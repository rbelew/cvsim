%% LyX 2.3.0 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[english]{achemso}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{babel}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{graphicx}
\usepackage{setspace}
\doublespacing
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=false,colorlinks=false]
 {hyperref}
\hypersetup{
 urlcolor=black}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.

\title{Generalized modeling of interventions across 78 countries}
\newcommand{\noun}[1]{\textsc{#1}}
%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\numberwithin{equation}{section}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.

\usepackage[nomarkers,figuresonly]{endfloat}

\author{Richard K. Belew}
\email{rbelew@ucsd.edu}
\affiliation{Univ. California -- San Diego, La Jolla CA}

\author{Cliff Kerr}
\email{ckerr@idmod.org}
\affiliation{Institute for Disease Modeling, Bellevue, WA}

\author{Jasmina Panovska-Griffiths}
\email{j.panovska-griffiths@ucl.ac.uk}
\affiliation{University College London}

\author{Dina Mistry}
\email{dmistry@idmod.org}
\affiliation{Institute for Disease Modeling, Bellevue, WA}

\SectionNumbersOn

\pagestyle{myheadings}
\markboth{Generalized models of 78 countries - Draft 8 Mar 20}{Generalized models of 78 countries - Draft 8 Mar 21}

% \DeclareBibliographyCategory{cited}
% \AtEveryCitekey{\addtocategory{cited}{\thefield{entrykey}}}
% \usepackage{filecontents}
% \nocite{*}

\@ifundefined{showcaptionsetup}{}{%
 \PassOptionsToPackage{caption=false}{subfig}}
\usepackage{subfig}
\makeatother

\begin{document}
\noindent \begin{center}
\emph{Draft: 8 Mar 2021}
\par\end{center}
\begin{abstract}
COVID-19 continues to spread around the world and modeling plays an
important role in informing policy \cite{Press1015}. An individual-based
model called Covasim has recently been fit to data regarding confirmed
cases and deaths experience in the United Kingdom during the first
half of 2020, and then used it to evaluate alternative intervention
strategies there \cite{jpg20}. We extend this methodology to consider
data from 78 countries for which both intervention data and data concerning
testing rates is available to retrospectively model interventions
employed in the first months of 2020 in these countries. Because the
age distribution of populations is a key feature of the COVID-19 pandemic
and contacts among young people are often age-stratified and may play
an especially important role, we focus here on school closure interventions. 

A Covasim model was built for each country, with its population, age
distribution, disease emergence and school closing dates specified.
The model was then calibrated against data concerning COVID-19 diagnosed
cases and related deaths. All countries' simulations were run with
and without school interventions. Because assumptions regarding background
testing rates being performed are critical to statistics regarding
the number of diagnosed patients, three conditions were considered:
the same \emph{fixed} testing rated was assumed for all countries;
\emph{searching} for an estimated testing rate as part of model calibration;
the actual \emph{data} available. The goal of this \emph{post hoc}
analysis of historical data is to understand the limits of our modeling
tools as we move forward to use them for predictive tasks. We consider
the hypothesis: Does incorporating the additional knowledge of school
interventions into a model makes it a better predictor of data concerning
diagnoses and deaths? 

The calibration of model parameters plays an important role in their
evaluation, and an analysis of methods used to calibrate models is
considered in detail. First, experiments using global optimization
methods are used to search for model parameters that allowed a model
to best fit available data. Second, Approximate Bayesian Computation
(ABC) techniques were used to find model parameters with maximal posterior
probability according to kernal density estimates. ABC methods allow
estimation of our confidence in the calibrated models' parameters,
and they suggest we can \emph{not} reject the null hypothesis:  adding
school interventions can not be shown to improve the model.
\end{abstract}

\section{Introduction}

COVID-19 continues to spread around the world and modeling plays an
important role in informing policy \cite{Press1015}. Here we use
a broad range of countries' experiences to identify robust results
common across them. Comparisons between countries are essential for
the control of COVID-19 \cite{10.1001/jama.2020.6585}

\begin{quotation}

Although international comparisons are often disparaged because of
different data quality and fears of the \textquoteleft ecological
fallacy\textquoteright , if done carefully they can play a major role
in our learning what works best for controlling COVID-19. \cite{10.1093/ije/dyaa108}

... the COVID-19 epidemic shows the need for epidemiology to go back
to its roots---thinking about populations. Studying disease occurrence
by person, place and time (often referred to as \textquoteleft descriptive
epidemiology\textquoteright ) is usually taught in introductory courses,
even if this approach is then paid little attention subsequently.
COVID-19 is a striking example of how we can learn a great deal from
comparing countries, states, regions, time trends and persons, despite
of all the difficulties. \cite{10.1093/ije/dyaa108}

\end{quotation}

\begin{quotation}

Predictive models for large countries, such as the US, are even more
problematic because they aggregate heterogeneous sub-epidemics in
local areas.... Models should also seek to use the best possible data
for local predictions. \cite{10.1001/jama.2020.6585}

\end{quotation}

Covasim is a stochastic agent-based simulator for performing COVID-19
analyses. It depends on two primary parameters, \noun{initInfect},
the number of people initially infected at the start of the contagion,
and a baseline value for $\beta$, the transmission rate from susceptible
to infected individuals. A key feature of Covasim is its modeling
of interaction network ``layers'' distinguishing various settings
(work, school, transport, households, communities) which modulate
$\beta$ differently in these settings. 

With travel restrictions as they were in the spring and summer of
2020, levels of migration across national borders were considerably
smaller than that across state or provincial boundaries. Given the
data available, only models at the level of individual countries and
interventions ordered \emph{nationally} are considered here.\footnote{The absence of a national school closure policy in the United States
is the reason it is not included in this analysis.} Because the age distribution of populations is a key feature of the
COVID-19 pandemic and contacts among young people are often age-stratified
and may play an especially important role, we focus here on school
closure interventions. 

Table \ref{fig:countrySumm} lists the 78 countries included in this
study with their ISO-3 code and name, together with key statistics:
\begin{itemize}
\item population in 2019, from ECDC 
\item the start date for their simulation (when the number of infection
first went > 50)
\item number of days with data regarding testing
\item number of days closed by school interventions, together with the starting
and ending dates
\end{itemize}
Note that the countries modeled varied in population from just over
a million (EST) to IND with a population of 1.4 billion.

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/countrySummTbl}
\par\end{centering}
\caption{\label{fig:countrySumm}Country summary statistics}
\end{figure}

To give a preview of the experiments to be presented, consider the
comparison of two runs presented in Figure \ref{fig:comp-noSchool-HUN}.
This shows six curves summarizing two simulation run for the country
of Hungary (ISO3 code HUN). The two curves in the top left graph show
the simulation's estimate (solid line) and data (boxes) of cummulative
diagnoses during early 2020, the middle graph shows estimated vs.
data on cummulative deaths, and the lowest graph shows the cummulative
number of tests performed. These same curves are repeated for two
different experimental conditions, in this case simulations with and
without school interventions included in the model. Better models
are those whose simulated estimates for diagnoses and deaths have
the best match with available data (cf. Section \ref{subsec:Covasim}).
Section \ref{subsec:dataTR} contrasts models using different bases
for testing, and Section \ref{subsec:Incorporating-school-closure}
contrasts models with and without school closure interventions in
the model. 

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/vdeep_noVSchool_210307-HUN}
\par\end{centering}
\caption{\label{fig:comp-noSchool-HUN}Model predictions for HUN, without and
with intervention}
\end{figure}

On their face, because the right simulation's estimates of deaths
is a closer match to data, these results make it seem as if incorporating
school closures in the model is an improvement. This would make us
believe that modeling interventions is worthwhile, and that the model
parameter values ($\beta$ and \noun{initInfect}) associated with
this model should be considered more accurate. But is the difference
between the two models \emph{statistically significant}? Typically,
global optimization of simulation parameters is performed to find
values which minimize model mismatch \cite{10.1371/journal.pcbi.1007893}.
In Section \ref{sec:Appen-CMA-ES-search} we describe the optimization
of Covasim parameters using Covariance matrix adaptation evolution
strategy (CMA-ES). In Section \ref{subsec:calibration} these results
are also put in the probabilistically more sound context of Approximate
Bayesian Computation (ABC). 

As the database supporting infections, deaths and school interventions
during the first months of 2020 recedes into the past, it becomes
increasingly irrelevant to the pandemic situation now. The goal of
this \emph{post hoc} analysis over the broad set of historical data
across many countries is to understand the limits of our modeling
tools as we move forward to use them for analysis of newer data and
predictive purposes.

\section{Results}

The first section below reports on ``base model'' experiments that
do \emph{not} include school intervention, which use \emph{data} for
the testing rate, and which has been calibrated using ABC methods.
Experiments contrasting these models with ones \emph{searching} for
the testing rate are then presented in are in Section \ref{subsec:dataTR},
ones including school interventions in Section \ref{subsec:Incorporating-school-closure},
and ones calibrated using CMA-ES global optization in Section \ref{sec:Appen-CMA-ES-search}
An analysis of the statistical significance of these differences is
considered in Section \ref{subsec:calibration}.

\subsection{Base model}
\begin{itemize}
\item use of ABC on noSchool, trate=data as base case
\item Example country
\item contrasted with trate=srch, school, CMAES
\end{itemize}
Covasim defines the value of a model's fit to be a weighted sum of
the error between its predicted number of diagnoses and deaths related
to COVID-19 and data for these values. Figure \ref{fig:base-fit}
shows the range of resulting model fits across 78 countries, with
lower fitness scores assigned to simulations with a better match of
predictions for the number of diagnoses and deaths. Most countries
have fit values below 150, and the average fit across countries is
91.6, including the poor fit examples of UGA and SGP.

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/vdeep-allModelsFit}
\par\end{centering}
\caption{\label{fig:base-fit}Base model fits}
\end{figure}

Figure \ref{fig:base-examples} shows the result of two simulation
for COL and SGP; COL fit is the best, and SGP is one of the worst.
For each country, the top two plots capture model behavior relative
to data for the number of diagnoses across the time interval considered.
The bottom plot shows the number of tests that would have been given
over time. 

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/vdeep-minMaxFit}
\par\end{centering}
\caption{\label{fig:base-examples}Base model: COL and SGP}
\end{figure}

Figure \ref{fig:base-keyParam} shows the values of the key \noun{initInfect}
and $\beta$ parameters for all countries. Best KDE estimates for
\noun{initInfect} vary by three orders of magnitude, and those for
$\beta$ vary by a factor of three.\footnote{KDE \noun{initInfect}: SGN=1137; TUR=45026. KDE $\beta$: PSE=0.0016;
GHA=0.0059 }.

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/vdeep-keyParam}
\par\end{centering}
\caption{\label{fig:base-keyParam}Base model key parameter distribution}
\end{figure}


\subsection{Using available data for testing rate\label{subsec:dataTR}}

A central issue in epidemiological modeling concerns assumptions regarding
the background testing rates on which diagnoses counts are based.
Experiments over countries with data on testing rates (daily testing
counts as a fraction of the country's population) via OWID were contrasted
with average testing rates found via calibration. Figure \ref{fig:dataVsearchTrate}
contrasts the testing rate found via calibration search vs. the data-based
rate

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/compdataSearch2_210227}
\par\end{centering}
\caption{\label{fig:dataVsearchTrate}Testing rate from data vs. via search}
\end{figure}

The testing rates identified via calibration were generally much larger
(20x) than the data-based values. Two countries, ARE and BHR, had
much higher testing rates reported in the data than the others, with
little difference reflected by their calibrated values. None of the
models' fit changed appreciably (+/-200) using one or the other source
of testing rate. As a baseline, these results were also compared to
ones using the same, constant testing rate (as a fraction of the population
tested per day) of \noun{testing\_rate}=3.3e-4, the average across
all countries with data (shown as a red line in Figure \ref{fig:dataVsearchTrate}).
Again, simulations using this value for testing rate did not appreciably
change model fit from ones using data-based values.

Note that the difference between predicted testing rate and data regarding
this value was not used by the calibration process in these experiments
to guide the search for model parameters, although Covasim's Fit measure
could be modified to do so. 

\subsection{Incorporating school closure interventions\label{subsec:Incorporating-school-closure}}

The next set of experiments contrast runs with and without school
closure interventions included in the model. As shown in Figure \ref{fig:intrvnFitDiff},
most models' fits were not changed much by including school intervention.
Since fit is measured using a scale where smaller values mean better
fit, negative differences indicate models which were improved with
interventions modeled, and positive differences indicate poorer fit
when interventions are included. When model fit changed, the model
\emph{without} school intervention modeled was typically superior.

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/vdeep_noVSchool_210307-fitDiff}
\par\end{centering}
\caption{\label{fig:intrvnFitDiff}Change in fitness with intervention}
\end{figure}

Figure \ref{fig:intrvn-goodEg} shows the curves for two countries,
HUN (highlighted green in Figure \ref{fig:intrvnFitDiff}) and ITA
(red) for which incorporation of school interventions decreased (i.e.,
improved) or increased (harmed) their fit most. The dashed lines in
the right-hand figures reflect the beginning and ending dates of the
interventions.

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/vdeep_noVSchool_210307-HUN+ITA}
\par\end{centering}
\caption{\label{fig:intrvn-goodEg}Intervention changes for HUN and ITA}
\end{figure}


\subsection{Model calibration using ABC and CMA-ES\label{subsec:calibration}}

\begin{quotation}

Calibration is a complex and dark art and cannot be covered fully
here; many books have been written about it and it continues to be
an area of active research. A good review article about calibrating
agent-based models like Covasim is available here \cite{10.1371/journal.pcbi.1007893}.
Calibration is usually expressed as an optimization problem: specifically,
find a vector of parameters $\theta$ that minimizes the mismatch
between the data D and the model $M(\theta)$. \href{https://github.com/InstituteforDiseaseModeling/covasim/blob/master/docs/tutorials/t7.ipynb}{Covasim Tutorial\#7}

\end{quotation}

Computational complexity theory ``...help{[}s{]} us comprehend why
so many practical problems seem to be resistant to efficient solution
by computer'' and warns that for ``total search'' problems such
as this, there can be no guarantee of finding the actual optimum value
\cite{Papadimitriou15881}. 

Initial experiments calibrating Covasim with school intervention data
were done using CMA-ES. The goal of providing a statistical foundation
for model parameters found via calibration, however, subsequently
lead to the use of ABC methods as described in Section \ref{subsec:Calibration-using-ABC}.
The ABC method is motivated there, along with experiments calibrating
a simpler compartmental ODE model to build intuition about ABC search.
The values found by ABC calibration are the ones described in all
preceding experiments. Section \ref{subsec:Statistical-significance}
addresses the statistical signfiicance of the observed differences
across experiments.

Here we begin by putting all the various experimental results into
a common frame by considering their differing values for the calibrated
parameters $\beta$ and \noun{initInfect}. Figure \ref{fig:allCalibParam-HUN}
focuses on HUN, the same example mentioned in the introduction and
Section \ref{subsec:Incorporating-school-closure}. The KDE for these
parameters $\beta$ and \noun{initInfect} found by ABC search are
represented by the highlighted red point. Three other points showing
the parameter values $\beta$ and \noun{initInfect} found and their
respective fitnesses in their labels are also shown:
\begin{itemize}
\item using search to establish the testing rate (vs. the base model's use
of data);
\item incorporating school interventions (vs. not); and 
\item using CMA-ES to discover parameter values (vs. ABC)
\end{itemize}
The \noun{pyABC} library does not explicitly associate a fitness value
with the KDE parameter estimate\footnote{Specifically: pyabc.visualization.credible.compute\_quantile(vals,
w, 0.5)} and so the \emph{minimum} fitness point found during ABC search is
also shown, and labeled with its fitness. The fitness values shown
in the other labels is similarly taken from the corresponding minimum
fitness.

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/vdeep-compAll-HUN-noErrBar}
\par\end{centering}
\caption{\label{fig:allCalibParam-HUN}Calibrated parameters - HUN}
\end{figure}

Note that these data are plotted on logarithmic scales, and that all
of the pairwise differences are large. 

This example (HUN) might make it appear as if the ABC-based experiments
cluster more closely with one another than with the values found by
CMA-ES. Figure \ref{fig:modelComp-scatter} displays all $\beta$
and \noun{initInfect} determined using the five measures above, and
Figure \ref{fig:modelCompFit} shows the models' fit values. Again,
it is tempting to look for patterns (e.g., models with school closure
interventions made use of particularly high values of $\beta$, CMA-ES
calibration made use of lower \noun{initInfect}), but there does not
seem to be any general pattern distinguishing any of the experimental
conditions. The next section helps to explain why.

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/vdeep-comp-modelScatter}
\par\end{centering}
\caption{\label{fig:modelComp-scatter}Alternative model parameter values}
\end{figure}

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/vdeep-comps-modelFit}
\par\end{centering}
\caption{\label{fig:modelCompFit}Alternative model fit}
\end{figure}


\subsection{Statistical significance\label{subsec:Statistical-significance}}

A key advantage of ABC methods is that they can provide kernel density
estimates of the parameters they find that minimize model fit. For
SMC-ABC this requires weighting samples according to their ``importance''
and then computing credible intervals of some confidence.

Figure \ref{fig:allCalibParam-HUN-errBar} shows the same data points
for HUN as in Figure \ref{fig:allCalibParam-HUN}, together with confidence=0.9
error bars on the $\beta$ and \noun{initInfect }of the KDE estimate.
For HUN, and for every other country modeled, the confidence intervals
of the best KDE estimates far exceed differences between any of the
experimental conditions.

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/vdeep-compAll-HUN-errBar}
\par\end{centering}
\caption{\label{fig:allCalibParam-HUN-errBar}Calibrated parameters with credible
intervals - HUN}
\end{figure}


\subsection{Sensitivity analysis}

To test whether the KDE parameters associated with calibrated models
were sufficient to distinguish models with and without school interventions,
a separate set of experiments were run to replicate model predictions
near these values. For each country, final $\beta$ and \noun{initInfect
}values were taken (cf. Figure \ref{fig:modelComp-scatter}) and a
small amount (1\%) noise was added to these for each of a set of replicated
runs near these values, for models both with and without school interventions
included. The results are shown in Figure \ref{fig:intrvnImpact},
showing the averge number of cummulative infections (normalized by
population) with standard deviation bars over the set of replicated
experiments. Most countries show very low normalized infection levels
in either case. For countries with signficant infection numbers, some
countries (e.g., CHL) show the expected pattern of reduced infections
with school interventions, many countries (e.g., IRN) show the opposite
change, and many others (e.g., KWT) show that the two models cannot
be discriminated. These differences could not be attributed to the
number of days of school closure (cf. Figure \ref{fig:countrySumm})
(data not shown).

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.8\paperwidth]{figs/interventionImpact}
\par\end{centering}
\caption{\label{fig:intrvnImpact}Intervention impact}
\end{figure}


\section{Methods}

\subsection{Data sources\label{subsec:Data-sources}}

The ECDC was used as the primary organizing data source. ISO3 codes
were used to identify and merge country data.\footnote{Two countries missing from ECDC lists, TWN and HNG. 2019 population
populations for these two was obtained from 'pop19': 23773876 \}\href{https://www.worldometers.info/world-population/}{WorldOMeters} }
\begin{itemize}
\item \href{https://www.ecdc.europa.eu/sites/default/files/documents/COVID-19-geographic-disbtribution-worldwide-2020-08-04.xlsx}{ECDC}
\item \href{ttps://github.com/owid/covid-19-data/blob/master/public/data/testing/covid-testing-all-observations.csv}{OWID testing}\cite{owidcoronavirus}
\item \href{http://en.unesco.org/sites/default/files/covid_impact_education.csv}{UNESCO interventions}
\item \href{http://github.com/OxCGRT/covid-policy-tracker/blob/master/data/OxCGRT_latest_responses.csv}{OxCGRT interventions}
\item Age distribution data from the \href{https://github.com/neherlab/covid19_scenarios/blob/master/src/assets/data/country_age_distribution.json}{Neher Lab}
as distributed as part of Covasim\footnote{cf. \texttt{covasim.data.country\_age\_data.py}}
\end{itemize}
\begin{figure}
\noindent \begin{centering}
\includegraphics[height=3in]{figs/dataSources-v2}
\par\end{centering}
\caption{\label{fig:Data-sources}Data sources}
\end{figure}

Only countries with populations greater than one million were considered,
and the start date for each countries' simulation was picked to begin
when data showed the number of infections went above 50.

\subsubsection{School interventions}

Preliminary experiments used a database of interventions developed
by a database of international intervention specifics called \href{https://github.com/amel-github/covid19-interventionmeasures}{Covid-19 Control Strategies List},
developed by Amélie Desvars-Larrive and colleagues {[}\href{https://www.csh.ac.at}{Complexity Science Hub Vienna}{]}.
This data includes a fine-grained analysis of which school levels
(kindergarten, primary, secondary, university) were ordered closed,
and therefore supported fine-grained variations in the age distributions
of the sort used effectively by Covasim's population generator. 37
countries were included in this data.

The experiments reported here used instead a \emph{consensus} database
of school closures coming from two distinct data sources, OxCGRT and
UNESCO. While broadly consistent, their respective accounts as to
just when individual countries were closed nationally was not identical.
Figure \ref{fig:overlapIntervnData} shows two countries' examples:
BHR, a country with exactly overlapping accounts and DNK with more
discordant ones. The graphs each show two plots of when school closures
were in effect (a value of 1, vs. 0 when not in effect) as described
by a source.

\begin{figure}
\noindent \begin{centering}
\includegraphics[width=0.4\paperwidth]{figs/BHR_ox-unesco_educIntrvn}
\includegraphics[width=0.4\paperwidth]{figs/DNK_ox-unesco_educIntrvn}
\par\end{centering}
\noindent \centering{}\caption{\label{fig:overlapIntervnData}Overlapping intervention data}
\end{figure}

The specification of school closures used in these experiments was
conservative: Only intervention periods that were identified by \emph{both}
sources, for periods > 14 days were considered. 78 countries had school
interventions that satisfied these criteria.

\subsubsection{Testing rate data}

Testing data from OurWorldInData \cite{owidcoronavirus} was used.
For the comparisons in Section \ref{subsec:dataTR}, this daily testing
data was converted to an average testing rate using the country's
population and averaging across the time period of the simulation.

\subsection{Covasim\label{subsec:Covasim}}

Covasim \cite{Kerr2020.05.10.20097469} is an open-source agent-based
(a.k.a. individual-based) model that uses demographic information
on age structure and population size to build realistic transmission
networks within distinct social layers. It allows age-specific disease
outcomes, intrahost viral dynamics, and the ability to incorporate
many sorts of interventions affecting these model elements. 

The basic model requires specification of two key parameters, the
initial number of infected individuals in the population, and $\beta$,
the transmission rate from susceptible to infected individuals. The
global $\beta$ parameter is proposed by the calibration optimizer,
then enhanced/attenuated across individual network layers according
to parameters based loosely on time-use surveys that track how many
hours a week people spend in various settings (work, school, transport,
etc), with households getting an additional multiplier for closeness
of contact. 

Better models are those whose simulated estimates for diagnoses and
deaths are closer to available data. In experiments here we used Covasim's
default \href{https://docs.idmod.org/projects/covasim/en/latest/tutorials/t7.html?highlight=fit\#The-Fit-object}{Fit measure}
(normalized absolute difference) of mismatch. As data regarding deaths
is believed to generally more accurate than that about positive diagnoses,
Covasim uses a weighted sum of error with death rates weighted twice
as heavily as diagnoses.

School closures are modeled as a simple on/off: school closings start/stop
on a specified dates, and layer-specific values for $\beta$ are enhanced
or attenuated. Following \cite{jpg20}, infection rate was set very
low within the schools level, increased within the home level, and
lowered significantly in work and community levels. The net values
for $\beta$ incorporating both default Covasim values and the result
of school interventions are shown in Table \ref{tab:beta-levels}.

\begin{table}
\noindent \begin{centering}
\begin{tabular}{|c|c|c|c|}
\hline 
$\beta$  & Covasim & Intervention & Net with intervention\tabularnewline
\hline 
\hline 
Home & 3.00 & 1.29 & 3.87\tabularnewline
\hline 
School & 0.60 & 0.02 & 0.01\tabularnewline
\hline 
Work & 0.60 & 0.20 & 0.12\tabularnewline
\hline 
Community & 0.30 & 0.20 & 0.06\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
\caption{\label{tab:beta-levels}Per-level $\beta$ values}
\end{table}


\subsection{Calibration using CMA-ES\label{subsec:Calibration-using-CMA-ES}}

Covariance matrix adaptation evolution strategy (CMA-ES) \cite{hansen2016cma}
is a library for stochastic optimization over continuous domains of
non-linear, non-convex functions. The ``ES'' refers to ``Evolutionary
strategy,'' a type of evolutionary computation using the metaphors
of populations and generations, and distinguished by adaptive rates
of mutation across generations. The CMA library has a ``tell-ask''
interface that allows easy bundling of parallel executions of a population
alternative values in each generation. The initial range of mutation
$\sigma_{0}$ was set to 0.1, and the initial search range for all
parameters was set to one quarter of the range between their upper
and lower bounds.

CMA-ES was used as one calibration outer-loop around the basic Covasim
simulation to search for key parameter values that allow the model
to best fit against data. Preliminary experiments across a large number
of trials on a single country (data not shown) identified 25 generations
of 32 individuals = 800 trials total were allocated to find the search
for parameter values causing the model to best fit the data. Optimization
is always over two parameters, $\beta$ and \noun{initInfect}, the
number of initial infections. Initial search bounds for the optimization
were set:
\begin{itemize}
\item \noun{initInfect}: best = 21000, lower bound =16000, upper bound=26000
\item $\beta$: best = .005, lower bound = 0.001, upper bound=0.01
\end{itemize}
In the experiments described in Section \ref{subsec:dataTR}, a third
parameter corresponding to testing rate is also included. Bounds on
its search were established using the data for the 78 countries available:
\begin{itemize}
\item testingRate: best=3.3e-4, lower bound = 2e-6, upper bound=3e-3
\end{itemize}

\subsection{Calibration using ABC\label{subsec:Calibration-using-ABC}}

Bayesian methods are distinguished by their inference of a posterior
distribution for some set of parameters $\theta$ from the likelihood
of observed data given these values $\pi(y|\theta)$ and estimation
of the parameters prior to using any data $\pi(\theta)$:

\[
\pi(\theta|y)\,\propto\pi(y|\theta)\,\pi(\theta)
\]

Many issues arise (e.g., determining the normalization constant) in
translating this probabilistic strategy into an empirical estimate,
but a deeper issue concerns models with ``hidden'' variables not
reflected in observable data. The basic Bayesian inference can be
extended to consider hidden variables $x$ using the joint likelihood
of the data with these variables $\pi(y,x|\theta)$:

\[
\pi(\theta,x|y)\,\propto\pi(y,x|\theta)\,\pi(\theta)
\]

Simulations like Covasim can, when given input parameters $\theta$,
provide samples of data and hidden variable values from the joint
likelihood function. Sampling methods like Markov Chain Monte Carlo
(MCMC) can then provide numerical estimates for the posterior distribution.
Approximate Bayesian Computation versions of MCMC (ABC-MCMC) proceed
by iteratively selecting ``particles'' (simulations run with parameters
$\theta_{i}$) that produce predictions $z_{i}$ that are required
to be closer $||y-z_{i}||<\epsilon$ to observed data, rejecting those
that are not, and resampling \cite{mckinley2018}. Sequential Monte
Carlo ABC (SMC-ABC) techniques specify a transition kernel for generating
new parameters to test and a method for reducing $\epsilon_{t}$ over
iterations \cite{10.1093/bioinformatics/btp619}.

\noun{pyABC} provides a Python implementation of SMC-ABC supporting
parallel execution of simulations on multicore hardware \cite{10.1093/bioinformatics/bty361}.
Simulations were given a uniform prior distribution over the same
bounds used for CMA-ES optimization. Other parameters control resources
allocated to SMC-ABC search such as the maximum number of ``generations''
of search reducing $\epsilon_{t}$, an absolute minimum on $\epsilon_{t}$,
etc.

Figure \ref{fig:pyABC-search} gives a sense of the search performed
by \noun{pyABC, }in this case for NZL. \ref{fig:pyABC-search}.a shows
the converging posterior estimates for $\beta$ and \noun{initInfect
}over the generations of search, and \ref{fig:pyABC-search}.b shows
the declining $\epsilon_{t}$ used to drive the search to better and
better fitting models.

\begin{figure}
\noindent \begin{centering}
\subfloat[{\protect\includegraphics[width=0.4\paperwidth]{figs/vdeep-NZL-history}}]{

} \subfloat[{\protect\includegraphics[width=0.4\paperwidth]{figs/vdeep-NZL-eps}}]{}
\par\end{centering}
\caption{\noun{\label{fig:pyABC-search}pyABC }search for NZL parameters}
\end{figure}


\section{Conclusions}

Experiments reported considered a standardized modeling approach applied
across a broad range of countries. The countries were selected because
they had available data on both nation-wide school closures and testing
rates during early, dynamic days of the pandemic in early 2020. It
was expected that incorporating school interventions into models would
allow them to better fit data concerning deaths and diagnoses, but
that was not found to be the case. Similarly, it was expected than
making use of data on testing rates within countries would lead to
better models than assuming a constant testing rate or allowing calibration
to set testing rates at levels that lead to better fitting models.
While calibrated testing rates did differ considerably from data-based
values, differences in the resulting models did lead to better fit
with deaths and diagnosis data. A comparison of global CMA-ES optimization
techniques to Bayesian SMC-ABC calibration showed broadly consistent
results in the parameter values discovered by the two methods. But
the explicit calculation of posterior probabilities allowed by the
SMC-ABC methods suggest that none of the observed variations across
experiments can be made with statistical confidence.

\pagebreak{}

\bibliographystyle{achemso}
\bibliography{covid}

\end{document}
